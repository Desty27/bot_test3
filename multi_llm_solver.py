import streamlit as st
import numpy as np
from PIL import Image
import os
import re
import subprocess
import sys
import json
from io import StringIO

# Set page config FIRST - before any other Streamlit commands
st.set_page_config(
    page_title="Enhanced Multi-LLM DSA Solver",
    page_icon="üöÄ",
    layout="wide"
)

# Try to import optional dependencies
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# Enhanced Tesseract Setup for Streamlit Cloud
try:
    import pytesseract
    
    # Streamlit Cloud and Linux systems typically have tesseract in /usr/bin/
    tesseract_paths = [
        '/usr/bin/tesseract',                    # Linux/Streamlit Cloud (primary)
        '/usr/local/bin/tesseract',              # macOS/Linux alternative
        'tesseract',                             # System PATH (try this first for Streamlit Cloud)
        r'C:\Program Files\Tesseract-OCR\tesseract.exe',        # Windows
        r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',  # Windows 32-bit
    ]
    
    tesseract_found = False
    tesseract_error = ""
    
    # First, try the default system installation (most likely to work on Streamlit Cloud)
    try:
        pytesseract.pytesseract.tesseract_cmd = 'tesseract'
        version = pytesseract.get_tesseract_version()
        tesseract_found = True
        tesseract_error = f"Found Tesseract version: {version}"
    except Exception as e:
        tesseract_error = f"Default tesseract failed: {str(e)}"
        
        # If default fails, try specific paths
        for path in tesseract_paths:
            try:
                if path != "tesseract":
                    pytesseract.pytesseract.tesseract_cmd = path
                
                # Test if tesseract is working
                version = pytesseract.get_tesseract_version()
                tesseract_found = True
                tesseract_error = f"Found Tesseract at {path}, version: {version}"
                break
            except Exception as e:
                tesseract_error += f" | {path}: {str(e)}"
                continue
    
    TESSERACT_AVAILABLE = tesseract_found
    
except ImportError as e:
    TESSERACT_AVAILABLE = False
    tesseract_error = f"pytesseract package not installed: {str(e)}"

try:
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import SystemMessage, UserMessage
    from azure.core.credentials import AzureKeyCredential
    AZURE_AI_AVAILABLE = True
except ImportError:
    AZURE_AI_AVAILABLE = False

try:
    from openai import AzureOpenAI
    AZURE_OPENAI_AVAILABLE = True
except ImportError:
    AZURE_OPENAI_AVAILABLE = False

# Configuration for Streamlit Cloud Deployment
try:
    # Get configuration from Streamlit secrets
    AZURE_API_KEY = st.secrets["AZURE_API_KEY"]
    INFERENCE_ENDPOINT = st.secrets["INFERENCE_ENDPOINT"]
    LLAMA_MODEL = st.secrets.get("LLAMA_MODEL", "Meta-Llama-3.1-405B-Instruct")
    CODESTRAL_MODEL = st.secrets.get("CODESTRAL_MODEL", "Codestral-2501")
    DEEPSEEK_MODEL = st.secrets.get("DEEPSEEK_MODEL", "DeepSeek-R1-0528")

    # Azure OpenAI Configuration
    GPT5_ENDPOINT = st.secrets["GPT5_ENDPOINT"]
    GPT5_API_KEY = st.secrets["GPT5_API_KEY"]
    GPT5_API_VERSION = st.secrets.get("GPT5_API_VERSION", "2024-12-01-preview")

    # GPT-4.1 Configuration
    GPT41_ENDPOINT = st.secrets["GPT41_ENDPOINT"]
    GPT41_API_KEY = st.secrets["GPT41_API_KEY"]
    GPT41_API_VERSION = st.secrets.get("GPT41_API_VERSION", "2024-12-01-preview")
    
except Exception as e:
    st.error(f"‚ö†Ô∏è Configuration Error: {str(e)}")
    st.error("Please ensure all required secrets are configured in Streamlit Cloud.")
    st.stop()

# Available models
GPT5_MODEL_NAME = "gpt-5"
GPT5_DEPLOYMENT = "gpt-5"
GPT41_MODEL_NAME = "gpt-4.1"
GPT41_DEPLOYMENT = "gpt-4.1"

# Try to import optional dependencies
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

@st.cache_resource
def get_inference_client():
    """Initialize Azure AI Inference client"""
    if not AZURE_AI_AVAILABLE:
        return None
    try:
        client = ChatCompletionsClient(
            endpoint=INFERENCE_ENDPOINT,
            credential=AzureKeyCredential(AZURE_API_KEY)
        )
        return client
    except Exception as e:
        st.error(f"Inference client error: {str(e)}")
        return None

@st.cache_resource
def get_gpt5_client():
    """Initialize GPT-5 Azure OpenAI client"""
    if not AZURE_OPENAI_AVAILABLE:
        return None
    try:
        from openai import AzureOpenAI
        client = AzureOpenAI(
            api_version=GPT5_API_VERSION,
            azure_endpoint=GPT5_ENDPOINT,
            api_key=GPT5_API_KEY,
        )
        return client
    except Exception as e:
        st.error(f"GPT-5 client error: {str(e)}")
        return None

@st.cache_resource
def get_gpt41_client():
    """Initialize GPT-4.1 Azure OpenAI client"""
    if not AZURE_OPENAI_AVAILABLE:
        return None
    try:
        from openai import AzureOpenAI
        client = AzureOpenAI(
            api_version=GPT41_API_VERSION,
            azure_endpoint=GPT41_ENDPOINT,
            api_key=GPT41_API_KEY,
        )
        return client
    except Exception as e:
        st.error(f"GPT-4.1 client error: {str(e)}")
        return None

def call_ai_model(prompt, model_name, system_message="You are an expert AI assistant.", max_tokens=4000):
    """Unified function to call any AI model"""
    
    # Handle GPT-5 through its specific client
    if model_name == GPT5_MODEL_NAME:
        client = get_gpt5_client()
        if not client:
            return f"Failed to initialize GPT-5 client for {model_name}"
        
        try:
            response = client.chat.completions.create(
                messages=[
                    {
                        "role": "system",
                        "content": system_message,
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    }
                ],
                model=GPT5_DEPLOYMENT
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error calling {model_name}: {str(e)}"
    
    # Handle GPT-4.1 through its specific client
    if model_name == GPT41_MODEL_NAME:
        client = get_gpt41_client()
        if not client:
            return f"Failed to initialize GPT-4.1 client for {model_name}"
        
        try:
            response = client.chat.completions.create(
                messages=[
                    {
                        "role": "system",
                        "content": system_message,
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    }
                ],
                model=GPT41_DEPLOYMENT,
                max_tokens=13107,
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error calling {model_name}: {str(e)}"
    
    # Handle Azure AI Inference models (Llama, Codestral, DeepSeek)
    if model_name in [LLAMA_MODEL, CODESTRAL_MODEL, DEEPSEEK_MODEL]:
        client = get_inference_client()
        if not client:
            return f"Failed to initialize AI client for {model_name}"
        
        try:
            response = client.complete(
                messages=[
                    SystemMessage(content=system_message),
                    UserMessage(content=prompt)
                ],
                model=model_name,
                temperature=0.3,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error calling {model_name}: {str(e)}"
    
    return f"Unknown model: {model_name}"

def extract_text_from_image(image):
    """Extract text from image using OCR with graceful fallback"""
    if not TESSERACT_AVAILABLE:
        return """
‚ö†Ô∏è OCR CURRENTLY UNAVAILABLE
        
Tesseract OCR is not installed on this Streamlit Cloud instance.

WORKAROUND: 
1. Use manual input below (fully functional!)
2. Copy/paste the problem text from the image
3. Or take a screenshot and type the problem manually

Your Enhanced Multi-LLM DSA Solver works perfectly with manual input!
All AI models and advanced features are available.
        """
        
    try:
        if CV2_AVAILABLE:
            img_array = np.array(image)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            denoised = cv2.fastNlMeansDenoising(gray)
            _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            processed_img = thresh
        else:
            processed_img = np.array(image.convert('L'))
        
        # Try multiple OCR configurations for better results
        configs = [
            r'--oem 3 --psm 6',  # Default config
            r'--oem 3 --psm 13', # Raw line. Treat the image as a single text line
            r'--oem 3 --psm 8',  # Single word
            r'--oem 3 --psm 7',  # Single text line
        ]
        
        for config in configs:
            try:
                text = pytesseract.image_to_string(processed_img, config=config)
                if text.strip():
                    return text.strip()
            except Exception:
                continue
                
        return "‚ùå No text detected in image. Please try manual input."
        
    except Exception as e:
        return f"‚ùå OCR Error: {str(e)}. Please use manual input instead."

def extract_python_code(response_text):
    """Extract Python code from response"""
    code_blocks = re.findall(r'```python\n(.*?)\n```', response_text, re.DOTALL)
    if code_blocks:
        return code_blocks[0]
    
    code_blocks = re.findall(r'```\n(.*?)\n```', response_text, re.DOTALL)
    if code_blocks:
        return code_blocks[0]
    
    return response_text

def filter_model_response(response_text, model_name):
    """Filter out any unwanted parts from model responses"""
    # Currently only handles DeepSeek R1 thinking parts, but can be extended for other models
    if "DeepSeek-R1" not in model_name:
        return response_text
    
    # Remove thinking tags and content (case insensitive)
    filtered_text = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL | re.IGNORECASE)
    
    # Remove any remaining thinking markers
    filtered_text = re.sub(r'<thinking>.*?</thinking>', '', filtered_text, flags=re.DOTALL | re.IGNORECASE)
    
    # Remove any other thinking patterns
    filtered_text = re.sub(r'<Think>.*?</Think>', '', filtered_text, flags=re.DOTALL)
    
    # Clean up extra whitespace
    filtered_text = re.sub(r'\n\s*\n', '\n\n', filtered_text.strip())
    
    return filtered_text

def extract_code_with_fallback(response_text, model_name):
    """Extract code with fallback LLM if extraction has issues"""
    # First try normal extraction
    code = extract_python_code(response_text)
    
    # If no code found, try using another LLM to extract
    if not code:
        extraction_prompt = f"""
        Extract ONLY the Python code from this response. Remove any thinking parts or explanations.
        Return ONLY the clean Python code that can be executed directly.
        
        Response to extract from:
        {response_text}
        
        Provide only the Python code, nothing else.
        """
        
        # Try GPT-5, then GPT-4.1, then other models for extraction
        for extraction_model in [GPT5_MODEL_NAME, GPT41_MODEL_NAME, LLAMA_MODEL, CODESTRAL_MODEL]:
            try:
                extracted_response = call_ai_model(
                    extraction_prompt, 
                    extraction_model, 
                    "You are a code extraction specialist. Return only clean Python code.",
                    max_tokens=4000
                )
                if extracted_response and not str(extracted_response).startswith("Error calling"):
                    extracted_code = extract_python_code(extracted_response)
                    if extracted_code:
                        return extracted_code
            except:
                continue
    
    return code

def run_python_code(code):
    """Execute Python code and capture output"""
    try:
        import tempfile
        import uuid
        temp_file = f'temp_solution_{uuid.uuid4().hex[:8]}.py'
        
        with open(temp_file, 'w', encoding='utf-8') as f:
            f.write(code)
        
        result = subprocess.run(
            [sys.executable, temp_file],
            capture_output=True,
            text=True,
            timeout=30,
            encoding='utf-8'
        )
        
        if os.path.exists(temp_file):
            os.remove(temp_file)
        
        return result.stdout, result.stderr, result.returncode
    except subprocess.TimeoutExpired:
        return "", "Execution timed out (30 seconds)", 1
    except Exception as e:
        return "", str(e), 1

def deep_problem_analysis(problem_text, model_name):
    """Deep analysis of the problem using specified model"""
    system_message = "You are an expert competitive programming analyst. Provide detailed, structured analysis."
    
    prompt = f"""
    Perform a DEEP ANALYSIS of this DSA problem. Be extremely thorough.

    PROBLEM:
    {problem_text}

    Provide comprehensive analysis in this exact JSON format:
    {{
        "problem_type": "Specific problem category (e.g., Dynamic Programming, Graph, Array, etc.)",
        "difficulty_level": "Estimated difficulty (Easy/Medium/Hard)",
        "key_insights": [
            "Critical insight 1",
            "Critical insight 2",
            "Critical insight 3"
        ],
        "algorithm_approaches": [
            {{
                "name": "Approach name",
                "time_complexity": "O(n) notation",
                "space_complexity": "O(n) notation",
                "pros": "Advantages",
                "cons": "Disadvantages"
            }}
        ],
        "edge_cases": [
            "Specific edge case 1",
            "Specific edge case 2",
            "Specific edge case 3"
        ],
        "input_constraints": {{
            "size_limits": "Array/string size limits",
            "value_ranges": "Number value ranges",
            "special_conditions": "Any special input conditions"
        }},
        "optimal_approach": {{
            "algorithm": "Best algorithm name",
            "reason": "Why this approach is optimal",
            "implementation_hints": [
                "Hint 1 for implementation",
                "Hint 2 for implementation"
            ]
        }}
    }}

    Be specific and actionable in your analysis.
    """
    
    return call_ai_model(prompt, model_name, system_message, max_tokens=6000)

def generate_solution_with_model(problem_text, analysis, model_name, iteration_num):
    """Generate solution using specific model with iteration-aware prompting"""
    
    system_messages = {
        GPT41_MODEL_NAME: "You are GPT-4.1, an expert competitive programming assistant focused on correctness and optimal solutions.",
        GPT5_MODEL_NAME: "You are GPT-5, an advanced expert competitive programming assistant with superior analytical capabilities and solution generation.",
        LLAMA_MODEL: "You are Meta-Llama-3.1-405B, an expert algorithm designer focused on correctness and efficiency.",
        CODESTRAL_MODEL: "You are Codestral-2501, specialized in generating clean, bug-free Python code for competitive programming.",
        DEEPSEEK_MODEL: "You are DeepSeek-R1, an advanced reasoning model with superior problem-solving capabilities."
    }
    
    iteration_context = {
        1: "This is the FIRST iteration. Focus on creating a solid, well-tested foundation.",
        2: "This is the SECOND iteration. The previous attempt likely had basic issues. Focus on correctness.",
        3: "This is the THIRD iteration. Previous attempts failed. Be more careful with edge cases and algorithm logic.",
        4: "This is FOURTH iteration. Multiple failures occurred. Completely rethink the approach.",
        5: "This is FIFTH+ iteration. Previous approaches failed significantly. Use a different algorithmic strategy."
    }
    
    context = iteration_context.get(iteration_num, iteration_context[5])
    
    prompt = f"""
    {context}

    PROBLEM:
    {problem_text}

    ANALYSIS:
    {analysis}

    CRITICAL REQUIREMENTS:
    1. Generate COMPLETE Python code that ACTUALLY WORKS
    2. Include exactly 10 comprehensive test cases covering ALL edge cases
    3. Use shorter variable names (a, b, c, i, j, k, n, m, etc.)
    4. NO comments in the code whatsoever
    5. Each test case must print "Test X: PASS" or "Test X: FAIL" with expected vs actual
    6. Include final summary: "Failed: X" or "All tests passed"
    7. Handle ALL edge cases from the analysis
    8. Make sure the algorithm is CORRECT, not just syntactically valid

    CODE STRUCTURE:
    ```python
    def solve(params):
        # Your solution logic here
        return result

    # Test cases
    def test():
        tests = [
            # 10 test cases with expected results
        ]
        f = 0
        for i, (inp, exp) in enumerate(tests):
            res = solve(*inp) if isinstance(inp, tuple) else solve(inp)
            if res == exp:
                print(f"Test {{i+1}}: PASS")
            else:
                print(f"Test {{i+1}}: FAIL Expected={{exp}} Got={{res}}")
                f += 1
        print(f"Failed: {{f}}")

    test()
    ```

    Focus on algorithmic correctness. Make it WORK, not just compile.
    """
    
    return call_ai_model(prompt, model_name, system_messages.get(model_name, "You are an expert programmer."), max_tokens=8000)

def analyze_failure_patterns(iteration_history):
    """Analyze failure patterns across iterations to provide strategic insights"""
    if not iteration_history:
        return "No iteration history available"
    
    analysis = "FAILURE PATTERN ANALYSIS:\n"
    analysis += f"Total iterations attempted: {len(iteration_history)}\n\n"
    
    # Analyze test case progression
    test_progressions = []
    for hist in iteration_history:
        stdout = hist.get('stdout', '')
        if stdout:
            passed = len([line for line in stdout.split('\n') if 'PASS' in line and 'Test' in line])
            failed = len([line for line in stdout.split('\n') if 'FAIL' in line and 'Test' in line])
            test_progressions.append((passed, failed))
    
    if test_progressions:
        analysis += "TEST CASE PROGRESSION:\n"
        for i, (passed, failed) in enumerate(test_progressions):
            analysis += f"Iteration {i+1}: {passed} passed, {failed} failed\n"
        
        # Check if we're making progress
        if len(test_progressions) > 1:
            first_passed = test_progressions[0][0]
            last_passed = test_progressions[-1][0]
            if last_passed <= first_passed:
                analysis += "\n‚ö†Ô∏è WARNING: No improvement in test cases. Algorithm approach may be fundamentally wrong.\n"
            else:
                analysis += f"\n‚úÖ PROGRESS: Improved from {first_passed} to {last_passed} passing tests.\n"
    
    # Analyze error patterns
    error_patterns = {}
    for hist in iteration_history:
        stderr = hist.get('stderr', '')
        if stderr:
            if 'SyntaxError' in stderr:
                error_patterns['syntax'] = error_patterns.get('syntax', 0) + 1
            elif 'TypeError' in stderr:
                error_patterns['type'] = error_patterns.get('type', 0) + 1
            elif 'IndexError' in stderr:
                error_patterns['index'] = error_patterns.get('index', 0) + 1
            elif 'KeyError' in stderr:
                error_patterns['key'] = error_patterns.get('key', 0) + 1
    
    if error_patterns:
        analysis += "\nERROR PATTERNS:\n"
        for error_type, count in error_patterns.items():
            analysis += f"- {error_type} errors: {count} times\n"
    
    # Strategic recommendations
    analysis += "\nSTRATEGIC RECOMMENDATIONS:\n"
    if len(iteration_history) >= 3:
        if any('syntax' in hist.get('stderr', '').lower() for hist in iteration_history[-2:]):
            analysis += "- Focus on syntax correctness, use simpler code structure\n"
        
        recent_outputs = [hist.get('stdout', '') for hist in iteration_history[-2:]]
        if all('FAIL' in output for output in recent_outputs if output):
            analysis += "- Algorithm logic is consistently wrong, try different approach\n"
            analysis += "- Consider: 1) Different data structure, 2) Different algorithm, 3) Reverse thinking\n"
    
    return analysis

def choose_next_model(iteration_num, previous_models_used):
    """Strategically choose the next model based on iteration and previous usage"""
    # All available working models
    all_models = [GPT5_MODEL_NAME, GPT41_MODEL_NAME, LLAMA_MODEL, CODESTRAL_MODEL, DEEPSEEK_MODEL]
    
    model_strategy = {
        1: GPT5_MODEL_NAME,      # Start with GPT-5 for superior capabilities
        2: GPT41_MODEL_NAME,     # Fallback to GPT-4.1 for reliability
        3: LLAMA_MODEL,          # Switch to Llama for different approach
        4: CODESTRAL_MODEL,      # Codestral for code generation
        5: DEEPSEEK_MODEL,       # DeepSeek for advanced reasoning
        6: GPT5_MODEL_NAME,      # Back to GPT-5 with different strategy
        7: GPT41_MODEL_NAME,     # GPT-4.1 again
        8: LLAMA_MODEL,          # Llama again with more focus
    }
    
    # For iterations beyond 8, cycle through all models
    if iteration_num > 8:
        return all_models[(iteration_num - 1) % len(all_models)]
    
    return model_strategy.get(iteration_num, GPT5_MODEL_NAME)

def enhanced_solve_with_multi_llm(problem_text, max_iterations=10):
    """Enhanced solving with strategic multi-LLM approach and deep analysis"""
    
    # Step 1: Deep Problem Analysis
    st.subheader("üß† Deep Problem Analysis")
    analysis_progress = st.progress(0)
    
    with st.spinner("Performing deep problem analysis..."):
        # Use GPT-5 first, then GPT-4.1 as fallback for analysis
        analysis_model = None
        analysis_result = None
        
        # Try GPT-5 first, then fallback to other models
        analysis_models = [GPT5_MODEL_NAME, GPT41_MODEL_NAME, LLAMA_MODEL]
        for model in analysis_models:
            analysis_result = deep_problem_analysis(problem_text, model)
            if analysis_result and not str(analysis_result).startswith("Error calling"):
                analysis_model = model
                break
        
        if not analysis_result or str(analysis_result).startswith("Error calling"):
            st.error("Failed to perform problem analysis")
            return
    
    analysis_progress.progress(1.0)
    
    # Display analysis
    st.success(f"‚úÖ Analysis completed using {analysis_model}")
    with st.expander("üìä Detailed Problem Analysis", expanded=True):
        st.text_area("Analysis Result", analysis_result, height=400)
    
    # Step 2: Multi-LLM Iterative Solution Generation
    st.subheader("üöÄ Multi-LLM Solution Generation")
    
    iteration_history = []
    current_code = None
    models_used = []
    
    # Create columns for real-time progress tracking
    col1, col2 = st.columns([2, 1])
    
    with col2:
        st.markdown("**üéØ Strategy Panel**")
        strategy_container = st.empty()
        model_container = st.empty()
        progress_container = st.empty()
    
    for iteration in range(1, max_iterations + 1):
        with col1:
            st.markdown(f"### üîÑ Iteration {iteration}")
            
            # Choose model strategically
            chosen_model = choose_next_model(iteration, models_used)
            models_used.append(chosen_model)
            
            # Update strategy panel
            with col2:
                strategy_container.markdown(f"**Current Strategy:**\nIteration {iteration}/{max_iterations}")
                model_container.markdown(f"**Using Model:**\n{chosen_model}")
                progress_container.progress(iteration / max_iterations)
            
            st.info(f"ü§ñ Using {chosen_model} for iteration {iteration}")
            
            # Generate solution with the chosen model
            with st.spinner(f"Generating solution with {chosen_model}..."):
                solution_response = generate_solution_with_model(
                    problem_text, analysis_result, chosen_model, iteration
                )
            
            if str(solution_response).startswith("Error calling"):
                st.error(f"Model call failed: {solution_response}")
                continue
            
            # Extract code from response
            filtered_response = filter_model_response(solution_response, chosen_model)
            current_code = extract_code_with_fallback(filtered_response, chosen_model)
            
            if not current_code:
                st.error("No code extracted from response")
                continue
            
            st.code(current_code, language='python', line_numbers=True)
            
            # Execute and analyze
            with st.spinner("Testing solution..."):
                stdout, stderr, returncode = run_python_code(current_code)
            
            # Store iteration results
            iteration_data = {
                'iteration': iteration,
                'model': chosen_model,
                'code': current_code,
                'stdout': stdout,
                'stderr': stderr,
                'returncode': returncode,
                'response': filtered_response  # Store filtered response instead of raw
            }
            iteration_history.append(iteration_data)
            
            # Display results
            result_col1, result_col2 = st.columns(2)
            
            with result_col1:
                st.markdown("**üì§ Output:**")
                if stdout:
                    st.text_area(f"Output {iteration}", stdout, height=200, key=f"out_{iteration}")
                    
                    # Check for success
                    failure_indicators = ["FAIL", "failed", "ERROR", "Exception"]
                    success_indicators = ["Failed: 0", "All tests passed"]
                    
                    has_failures = any(fail in stdout for fail in failure_indicators)
                    has_success = any(success in stdout for success in success_indicators)
                    
                    if has_success and not has_failures:
                        st.success("üéâ All test cases passed!")
                        
                        # Final solution display
                        st.subheader("üèÜ FINAL SOLUTION")
                        st.code(current_code, language='python', line_numbers=True)
                        
                        st.download_button(
                            label="üì• Download Solution",
                            data=current_code,
                            file_name="enhanced_dsa_solution.py",
                            mime="text/python"
                        )
                        
                        # Success metrics
                        st.subheader("üìä Success Metrics")
                        st.metric("Iterations Used", iteration)
                        st.metric("Models Used", len(set(models_used)))
                        st.metric("Success Rate", f"{(1/iteration)*100:.1f}%")
                        
                        return
                    elif has_failures:
                        st.warning(f"‚ö†Ô∏è Test failures detected in iteration {iteration}")
                else:
                    st.info("No output generated")
            
            with result_col2:
                st.markdown("**‚ùå Errors:**")
                if stderr:
                    st.text_area(f"Errors {iteration}", stderr, height=200, key=f"err_{iteration}")
                else:
                    st.success("No errors!")
            
            # Deep analysis of failures
            if iteration > 1:
                with st.expander(f"üîç Failure Analysis - Iteration {iteration}"):
                    failure_analysis = analyze_failure_patterns(iteration_history)
                    st.text(failure_analysis)
    
    # Final summary if max iterations reached
    st.subheader("üìã Final Summary")
    st.error(f"‚ùå Maximum iterations ({max_iterations}) reached without complete solution")
    
    # Show progression metrics
    if iteration_history:
        st.markdown("**üìà Progression Analysis:**")
        
        models_tried = list(set([h['model'] for h in iteration_history]))
        test_progression = []
        
        for hist in iteration_history:
            stdout = hist.get('stdout', '')
            if stdout:
                passed = len([line for line in stdout.split('\n') if 'PASS' in line and 'Test' in line])
                test_progression.append(passed)
            else:
                test_progression.append(0)
        
        col_x, col_y, col_z = st.columns(3)
        with col_x:
            st.metric("Models Tried", len(models_tried))
        with col_y:
            st.metric("Best Test Score", max(test_progression) if test_progression else 0)
        with col_z:
            st.metric("Final Test Score", test_progression[-1] if test_progression else 0)
        
        # Show best code if any tests passed
        if test_progression and max(test_progression) > 0:
            best_iteration = test_progression.index(max(test_progression))
            best_code = iteration_history[best_iteration]['code']
            
            st.subheader("ü•à Best Attempt")
            st.info(f"Iteration {best_iteration + 1} with {max(test_progression)} passing tests")
            st.code(best_code, language='python')
            
            st.download_button(
                label="üì• Download Best Attempt",
                data=best_code,
                file_name="best_attempt_solution.py",
                mime="text/python"
            )

def main():
    st.title("üöÄ Enhanced Multi-LLM DSA Solver with GPT-5")
    st.markdown("**Advanced AI Pipeline:** GPT-5 Analysis + Strategic Multi-Model Approach")
    
    # Enhanced System status with troubleshooting info
    with st.expander("üîß System Status & Troubleshooting"):
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            if TESSERACT_AVAILABLE:
                st.success("‚úÖ Tesseract OCR")
                st.caption(f"Debug: {tesseract_error}")
            else:
                st.warning("‚ö†Ô∏è OCR Unavailable")
                st.info("üí° **Solution:** Use manual input below - works perfectly!")
                with st.expander("üîç OCR Troubleshooting (Optional)"):
                    st.markdown("""
                    **Tesseract isn't installed on this Streamlit Cloud instance.**
                    
                    **Quick Fix Options:**
                    1. **Use Manual Input** ‚Üê Recommended! Works perfectly
                    2. Try redeploying the app (may fix package installation)
                    3. Contact Streamlit Cloud support if OCR is critical
                    
                    **Your app works fully without OCR!**
                    All AI models and DSA solving features are available.
                    """)
                    st.code(f"Debug info: {tesseract_error[:200]}...", language="text")
                with st.expander("ÔøΩ Tesseract Fix Steps"):
                    st.markdown("""
                    **For Streamlit Cloud:**
                    1. Ensure `packages.txt` contains:
                       ```
                       tesseract-ocr
                       tesseract-ocr-eng
                       ```
                    2. Check deployment logs for package installation errors
                    3. Try redeploying the app
                    4. If still fails, OCR will be disabled but manual input works
                    """)
        with col2:
            st.success("‚úÖ OpenCV" if CV2_AVAILABLE else "‚ö†Ô∏è Basic Processing")
        with col3:
            if AZURE_AI_AVAILABLE:
                st.success("‚úÖ Azure AI")
            else:
                st.error("‚ùå Azure AI")
                st.info("üìù **Fix:** Check requirements_enhanced.txt has azure-ai-inference")
        with col4:
            client = get_inference_client()
            if client:
                st.success("‚úÖ Models Ready")
            else:
                st.error("‚ùå Models Failed")
                st.info("üìù **Fix:** Add Azure credentials to Streamlit Cloud Secrets")
        
        # Troubleshooting section
        if not TESSERACT_AVAILABLE or not AZURE_AI_AVAILABLE or not get_inference_client():
            st.markdown("### üÜò **Troubleshooting for Streamlit Cloud:**")
            
            if not TESSERACT_AVAILABLE:
                st.markdown("""
                **‚ùå Tesseract OCR Issue:**
                1. Ensure `packages.txt` is in your repository root
                2. Content should be:
                   ```
                   tesseract-ocr
                   tesseract-ocr-eng
                   libtesseract-dev
                   libleptonica-dev
                   ```
                3. Redeploy the app after adding the file
                """)
            
            if not AZURE_AI_AVAILABLE:
                st.markdown("""
                **‚ùå Azure AI Issue:**
                1. Check `requirements_enhanced.txt` includes:
                   ```
                   azure-ai-inference>=1.0.0b4
                   azure-core>=1.29.0
                   azure-identity>=1.15.0
                   ```
                2. Redeploy after updating requirements
                """)
            
            if not get_inference_client():
                st.markdown("""
                **‚ùå Models Failed:**
                1. Go to Streamlit Cloud app settings ‚Üí Secrets
                2. Add these secrets:
                   ```toml
                   AZURE_API_KEY = "your-azure-api-key-here"
                   INFERENCE_ENDPOINT = "https://your-endpoint.services.ai.azure.com/models"
                   LLAMA_MODEL = "Meta-Llama-3.1-405B-Instruct"
                   CODESTRAL_MODEL = "Codestral-2501"
                   DEEPSEEK_MODEL = "DeepSeek-R1-0528"
                   GPT5_ENDPOINT = "https://your-gpt5-endpoint.openai.azure.com/"
                   GPT5_API_KEY = "your-gpt5-api-key-here"
                   GPT5_API_VERSION = "2024-12-01-preview"
                   GPT41_ENDPOINT = "https://your-gpt41-endpoint.cognitiveservices.azure.com/"
                   GPT41_API_KEY = "your-gpt41-api-key-here"
                   GPT41_API_VERSION = "2024-12-01-preview"
                   ```
                3. Restart the app
                """)
        
        # Show required secrets configuration
        if not AZURE_AI_AVAILABLE or not get_inference_client():
            st.markdown("### üìã **Required Streamlit Cloud Secrets Configuration:**")
            st.code("""
# Add these to your Streamlit Cloud app secrets:
AZURE_API_KEY = "your-azure-inference-api-key"
INFERENCE_ENDPOINT = "https://your-endpoint.services.ai.azure.com/models"
GPT5_ENDPOINT = "https://your-gpt5-endpoint.openai.azure.com/"
GPT5_API_KEY = "your-gpt5-api-key"
GPT41_ENDPOINT = "https://your-gpt41-endpoint.cognitiveservices.azure.com/"
GPT41_API_KEY = "your-gpt41-api-key"

# Optional (have defaults):
LLAMA_MODEL = "Meta-Llama-3.1-405B-Instruct"
CODESTRAL_MODEL = "Codestral-2501" 
DEEPSEEK_MODEL = "DeepSeek-R1-0528"
GPT5_API_VERSION = "2024-12-01-preview"
GPT41_API_VERSION = "2024-12-01-preview"
            """, language="toml")
    
    # Model availability check
    if AZURE_OPENAI_AVAILABLE and (get_gpt5_client() or get_gpt41_client()):
        st.subheader("ü§ñ Model Status Check")
        col1, col2, col3 = st.columns(3)
        
        test_models = [GPT5_MODEL_NAME, GPT41_MODEL_NAME, LLAMA_MODEL]
        for i, model in enumerate(test_models):
            with [col1, col2, col3][i]:
                with st.spinner(f"Testing {model}..."):
                    test_result = call_ai_model("Hello", model, "You are a test assistant", max_tokens=10)
                    if test_result and not str(test_result).startswith("üî¥") and not str(test_result).startswith("Error"):
                        st.success(f"‚úÖ {model}")
                    else:
                        st.error(f"‚ùå {model}")
                        st.caption(f"Error: {str(test_result)[:100]}...")
    
        # Show additional available models
        with st.expander("ÔøΩ Available Models"):
            st.success("‚úÖ GPT-5: Advanced problem analysis and solution generation")
            st.success("‚úÖ GPT-4.1: Reliable coding and problem solving")
            st.info("‚úÖ Meta-Llama-3.1-405B: Open source reasoning model")
            st.info("‚úÖ Codestral-2501: Code generation specialist")
            st.info("‚úÖ DeepSeek-R1-0528: Advanced reasoning model")
    
    elif AZURE_OPENAI_AVAILABLE:
        st.warning("‚ö†Ô∏è Azure OpenAI available but client initialization failed - check credentials")
    
    # Configuration sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        max_iterations = st.slider(
            "Max Iterations", 
            min_value=3, 
            max_value=20, 
            value=10,
            help="Maximum number of solution attempts with different models"
        )
        
        st.header("üéØ Strategy Options")
        show_detailed = st.checkbox("Show Detailed Analysis", value=True)
        show_code_progression = st.checkbox("Show Code Progression", value=True)
        
        st.header("üìù Manual Input")
        manual_text = st.text_area(
            "Enter DSA Problem:",
            placeholder="Type or paste your problem here...",
            height=150
        )
        
        if manual_text and st.button("üöÄ Solve with Enhanced Multi-LLM"):
            st.session_state['solve_enhanced'] = manual_text
    
    # Check for manual solve
    if 'solve_enhanced' in st.session_state:
        problem_text = st.session_state['solve_enhanced']
        del st.session_state['solve_enhanced']
        
        st.subheader("üìù Problem Input")
        st.text_area("Problem Text", problem_text, height=150)
        
        enhanced_solve_with_multi_llm(problem_text, max_iterations)
        return
    
    # OCR File uploader
    st.subheader("üì∏ Upload Problem Images")
    uploaded_files = st.file_uploader(
        "Upload problem images",
        type=['png', 'jpg', 'jpeg'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.subheader("üì∏ Uploaded Images")
        
        cols = st.columns(min(len(uploaded_files), 3))
        for i, uploaded_file in enumerate(uploaded_files):
            with cols[i % 3]:
                image = Image.open(uploaded_file)
                st.image(image, caption=f"Image {i+1}", use_column_width=True)
        
        if st.button("üîç Extract Text and Solve with Enhanced Multi-LLM", type="primary"):
            all_text = ""
            
            for i, uploaded_file in enumerate(uploaded_files):
                image = Image.open(uploaded_file)
                
                with st.spinner(f"Extracting text from Image {i+1}..."):
                    text = extract_text_from_image(image)
                
                if text and not text.startswith("OCR_PLACEHOLDER"):
                    st.success(f"‚úÖ Text extracted from Image {i+1}")
                    with st.expander(f"üìù Text from Image {i+1}"):
                        st.text(text)
                    all_text += f"\n\nImage {i+1}:\n{text}"
                else:
                    st.warning(f"‚ö†Ô∏è No text found in Image {i+1}")
            
            if all_text.strip():
                st.subheader("üîó Combined Problem Text")
                st.text_area("Extracted Text", all_text, height=200)
                enhanced_solve_with_multi_llm(all_text, max_iterations)
            else:
                st.error("‚ùå No text extracted. Use manual input instead.")

if __name__ == "__main__":
    main()